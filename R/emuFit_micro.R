
#' Fit radEmu model. Called by emuFit
#'
#' @param X a p x J design matrix
#' @param Y an n x p matrix of nonnegative observations
#' @param B starting value of coefficient matrix (p x J)
#' @param constraint_fn function g defining constraint on rows of B; g(B_k) = 0
#' for rows k = 1, ..., p of B.
#' @param maxit maximum number of coordinate descent cycles to perform before
#' exiting optimization
#' @param tolerance tolerance on improvement in log likelihood at which to
#' exit optimization
#' @param verbose logical: print information about optimization progress? Default is TRUE.
#' @param warm_start logical: begin from "warm start" obtained from linear regression on
#' transformed counts? Default is TRUE.
#' @param c1 numeric: value of constant in Armijo condition checked in backtracking line search
#' @param max_stepsize numeric: maximum sup-norm value of proposed step. Default is 0.5.
#' @param max_abs_B numeric: maximum value elements of B are allowed to take in absolute value.
#' Helps prevent optimization failure in larger problems. Defaults to 50.
#' @param use_working_constraint logical: set a column of B equal to
#' zero within optimization. Default is TRUE.
#' @param j_ref If use_working_constraint is TRUE, column index of column of B
#' to set to zero. Default is NULL, in which case this column is chosen to
#' maximize the number of nonzero entries of Y_j_ref.
#' @return A p x J matrix containing regression coefficients (under constraint
#' g(B_k) = 0)
#'
#' @export
emuFit_micro <-
  function(X,
           Y,
           B = NULL,
           constraint_fn = NULL,
           maxit = 250,
           tolerance = 1e-5,
           verbose = TRUE,
           warm_start = TRUE,
           c1 = 1e-4,
           max_stepsize = 0.5,
           max_abs_B = 50,
           use_working_constraint = TRUE,
           j_ref = NULL){
    #extract dimensions
    n <- nrow(Y)
    J <- ncol(Y)
    p <- ncol(X)


    # assign constraint if necessary
    if(is.null(constraint_fn)){
      constraint_fn <- (function(x) pseudohuber_center(x,0.1))
      constraint_grad_fn <- (function(x) dpseudohuber_center_dx(x,0.1))
    }


    #populate B if not provided
    if(is.null(B)){
      if(!warm_start){
      B <- matrix(0,
                  nrow = p,
                 ncol = J)
      } else{

        Y_start <- Y + 0.01*mean(Y)
        for(i in 1:nrow(Y_start)){
          Y_start[i,] <- log(Y_start[i,])
          Y_start[i,] <- Y_start[i,] - mean(Y_start[i,])
        }
        B <- matrix(nrow = p,ncol = J)
        
        ##Checking if design matrix is rank-deficient and soln_mat can be found
        if (qr(t(X)%*%X)$rank < ncol(X)){
          stop("Design matrix X inputted for the model is rank-deficient, preventing proper model fitting.
  This might be due to multicollinearity, overparameterization, or redundant factor levels included in covariates.
  Consider removing highly correlated covariates or adjusting factor levels to ensure a full-rank design. \n")
        }
        
        soln_mat <- qr.solve(t(X)%*%X,t(X))
        for(j in 1:J){
          B[,j] <- as.numeric(as.matrix(soln_mat%*%Y_start[,j,drop = FALSE]))
        }
      }
    }



    loop_js <- 1:J
    if(use_working_constraint){
      if(is.null(j_ref)){
      detection <- colSums(Y>0)

      which_to_omit <- which(detection == max(detection))
      if(length(which_to_omit)>1){
        totals <- colSums(Y[,which_to_omit])
        which_to_omit <- which_to_omit[which.max(totals)]
      }
      } else{
        which_to_omit <- j_ref
      }

      loop_js <- loop_js[-which_to_omit]

      for(k in 1:p){
        B[k,] <- B[k,] - B[k,which_to_omit]
      }
    } else{
      for(k in 1:p){
        B[k,] <- B[k,] - constraint_fn(B[k,])
      }
    }




    #create object to store log likelihoods at each iteration in
    lls <- numeric(maxit)

    #initiate z
    z <- update_z(Y,X,B)

    #compute initial mean
    log_mean <- X%*%B +
      matrix(z,ncol = 1)%*%matrix(1,ncol = J, nrow = 1)

    lls[1] <- sum(Y*log_mean - exp(log_mean))

    converged <- FALSE



    iter <- 1
    deriv_norm <- Inf
    B_diff <- Inf
    while(!converged){
      old_B <- B
      if(!is.null(j_ref)){
        shifty <- optim(rep(0,p),function(x) lil_ll(x,
                                                    B = B,
                                                    p = p,
                                                    X = X,
                                                    Y = Y,
                                                    J = J,
                                                    j_ref = j_ref),
                        method = "BFGS")
        
        shift <- shifty$par
        
        # print(signif(shifty$par,2))
        
        for(k in 1:p){
          B[k,-j_ref] <-  B[k,-j_ref] + shifty$par[k]
        }
        #   
        # }
        
        z <- update_z(X = X, Y = Y, B = B)
      }
      for(j in loop_js){
        # print(j)
        # llj <- function(x){
        #   Bj <- matrix(x,ncol = 1)
        #   log_mean <- X%*%Bj + z
        #   return(sum(Y[,j]*log_mean - exp(log_mean)))
        # }
        # grj <- function(x){
        #   Bj <- matrix(x,ncol = 1)
        #   log_mean <- X%*%Bj + z
        #   return(-1*apply(diag(as.numeric((Y[,j] - exp(log_mean))))%*%X,
        #                2,sum))
        # }
        update <- micro_fisher(X,Yj = Y[,j,drop= FALSE],Bj = B[,j,drop = FALSE],z,
                               stepsize = max_stepsize,
                               c1 = c1)

        update_norm <- max(abs(update))
        if(update_norm>max_stepsize){
          update <- max_stepsize*update/update_norm
        }

        B[,j] <- B[,j] + update

        if(!use_working_constraint){
        for(k in 1:p){
          B[k,] <- B[k,] - constraint_fn(B[k,])
        }}



      }

      # keep any elements of B from flying off to infinity
      B[B < -max_abs_B] <- -max_abs_B
      B[B > max_abs_B] <- max_abs_B
      z <- update_z(Y = Y,X = X,B = B)



      log_mean <- X%*%B +
        matrix(z,ncol = 1)%*%matrix(1,ncol = J, nrow = 1)
      lls[iter  +1] <- sum(Y*log_mean - exp(log_mean))

        deriv <- do.call(c,lapply(1:J,
                                  function(j) crossprod(X,Y[,j,drop = FALSE] - exp(log_mean[,j,drop = FALSE]))))

        deriv_norm  = sqrt(sum(deriv^2))

        B_diff <- max(abs(B - old_B)[abs(B)<(0.5*max_abs_B)])

      if(verbose){
      message("Scaled norm of derivative ",signif(sqrt((1/(n*J))*deriv_norm),4) )
        if(iter>1){
          message("Max absolute difference in elements of B since last iteration: ",signif(B_diff,3))
        }
        }

      # if(deriv_norm < tolerance){
      #   converged <- TRUE
      # }
        if(B_diff < tolerance){
          converged <- TRUE
        }


      if(iter > maxit){
        converged <- TRUE
        if(verbose){
        message("Iteration limit reached; exiting optimization.")
          }
      }
      iter <- iter + 1
    }

    if(use_working_constraint){
      for(k in 1:p){
        B[k,] <- B[k,] - constraint_fn(B[k,])
      }}

    z <- update_z(Y = Y,X = X,B = B)

    log_mean <- X%*%B +
      matrix(z,ncol = 1)%*%matrix(1,ncol = J, nrow = 1)
    lls[iter  +1] <- sum(Y*log_mean - exp(log_mean))

    deriv <- do.call(c,lapply(1:J,
                              function(j) crossprod(X,Y[,j,drop = FALSE] - exp(log_mean[,j,drop = FALSE]))))

    deriv_norm  = sqrt(sum(deriv^2))
    # print(deriv_norm)

      return(B)
  }

